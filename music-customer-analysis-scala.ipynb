{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Customer-Music Data using Apache Spark\n",
    "\n",
    "The original Drill & Tableau based tutorial is at https://mapr.com/blog/real-time-user-profiles-spark-drill-and-mapr-db/. I have converted them to Spark 2.4 Jupyter Scala Notebooks. In addition to that I have added many more Spark based Data Analysis sections, Side by Side Spark comparisons DF API and Spark SQL constructs to realize the same use case. Also used Jupyter Notebook for data visualization.\n",
    "\n",
    "A special section for working with RDDs is also included.\n",
    "\n",
    "Users are continuously connecting to the service and listening to tracks that they like -- this generates our main data set. The behaviors captured in these events, over time, represent the highest level of detail about actual behaviors of customers as they consume the service by listening to music. In addition to the events of listening to individual tracks, we have a few other data sets representing all the information we might normally have in such a service. In this post we will make use of the following three data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Data Set\n",
    "\n",
    "**Individual customers listening to individual tracks: (tracks.csv)** - a collection of events, one per line, where each event is a client listening to a track.\n",
    "\n",
    "This data is approximately 1M lines and contains simulated listener events over several months.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><strong>Field Name</strong></th>\n",
    "    <th>Event ID</th>\n",
    "    <th>Customer ID</th>\n",
    "    <th>Track ID</th>\n",
    "    <th>Datetime</th>\n",
    "    <th>Mobile</th>\n",
    "    <th>Listening Zip</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Type</strong></td>\n",
    "    <td>Integer</td>\n",
    "    <td>Integer</td>\n",
    "    <td>Integer</td>\n",
    "    <td>String</td>\n",
    "    <td>Integer</td>\n",
    "    <td>Integer</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Example Value</strong></td>\n",
    "    <td>9999767</td>\n",
    "    <td>2597</td>\n",
    "    <td>788</td>\n",
    "    <td>2014-12-01 09:54:09</td>\n",
    "    <td>0</td>\n",
    "    <td>11003</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The event, customer and track IDs tell us what occurred (a customer listened to a certain track), while the other fields tell us some associated information, like whether the customer was listening on a mobile device and a guess about their location while they were listening. With many customers listening to many tracks, this data can get very large and will be the input into our Spark job.\n",
    "\n",
    "**Customer information:** - information about individual customers.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><strong>Field Name</strong></th>\n",
    "    <th>Customer ID</th>\n",
    "    <th>Name</th>\n",
    "    <th>Gender</th>\n",
    "    <th>Address</th>\n",
    "    <th>ZIP</th>\n",
    "    <th>Sign Date</th>    \n",
    "    <th>Status</th>\n",
    "    <th>Level</th>\n",
    "    <th>Campaign</th>\n",
    "    <th>Linked with Apps?</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Type</strong></td>\n",
    "    <td>Integer</td>\n",
    "    <td>String</td>\n",
    "    <td>Integer</td>\n",
    "    <td>String</td>\n",
    "    <td>Integer</td>\n",
    "    <td>String</td>\n",
    "    <td>Integer</td>\n",
    "    <td>Integer</td>\n",
    "    <td>Integer</td>\n",
    "    <td>Integer</td>    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Example Value</strong></td>\n",
    "    <td>10</td>\n",
    "    <td>Joshua Threadgill</td>\n",
    "    <td>0</td>\n",
    "    <td>10084 Easy Gate Bend</td>\n",
    "    <td>66216</td>\n",
    "    <td>01/13/2013</td>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The fields are defined as follows:\n",
    "```\n",
    "Customer ID: a unique identifier for that customer\n",
    "Name, gender, address, zip: the customer’s associated information\n",
    "Sign date: the date of addition to the service\n",
    "Status: indicates whether or not the account is active (0 = closed, 1 = active)\n",
    "Level: indicates what level of service -- 0, 1, 2 for Free, Silver and Gold, respectively\n",
    "Campaign: indicates the campaign under which the user joined, defined as the following (fictional) campaigns driven by our (also fictional) marketing team:\n",
    "NONE - no campaign\n",
    "30DAYFREE - a ‘30 days free’ trial offer\n",
    "SUPERBOWL - a Superbowl-related program\n",
    "RETAILSTORE - an offer originating in brick-and-mortar retail stores\n",
    "WEBOFFER - an offer for web-originated customers\n",
    "```\n",
    "\n",
    "**Previous ad clicks: (clicks.csv)** - a collection of user click events indicating which ad was played to the user and whether or not they clicked on it.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><strong>Field Name</strong></th>\n",
    "    <th><strong>EventID</strong></th>\n",
    "    <th>CustID</th>\n",
    "    <th>AdClicked</th>\n",
    "    <th>Localtime</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Type</strong></td>\n",
    "    <td>Integer</td>\n",
    "    <td>Integer</td>\n",
    "    <td>String</td>\n",
    "    <td>String</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Example Value</strong></td>\n",
    "    <td>0</td>\n",
    "    <td>109</td>\n",
    "    <td>ADV_FREE_REFERRAL</td>\n",
    "    <td>2014-12-01 09:54:09</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The fields that interest us are the foreign key identifying the customer (CustID), a string indicating which ad they clicked (AdClicked), and the time when it happened (Localtime). Note that we could use a lot more features here, such as basic information about the customer (gender, etc.), but to keep things simple for the example we’ll leave that as a future exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame, Dataset, Row}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.sql.expressions.UserDefinedFunction\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.functions.{col, udf, asc, desc, when, array, struct}\n",
    "import org.apache.spark.sql.functions.{sum, avg, count, countDistinct, hour, lit, format_number, explode}\n",
    "\n",
    "import scala.collection.mutable.Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@6f4f4ba2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://ip-172-30-2-158.ec2.internal:4040)\" target=\"new_tab\">Spark UI: local-1547543707793</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1547543707793: Some(http://ip-172-30-2-158.ec2.internal:4040)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark: SparkSession = (SparkSession\n",
    "                           .builder\n",
    "                           .master(\"local[*]\")\n",
    "                           .appName(\"music-customer-analysis-with-spark\")\n",
    "                           .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the data from files into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MUSIC_TRACKS_DATA = data/tracks.csv\n",
       "CUSTOMER_DATA = data/cust.csv\n",
       "CLICKS_DATA = data/clicks.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "data/clicks.csv"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val MUSIC_TRACKS_DATA: String = \"data/tracks.csv\"\n",
    "val CUSTOMER_DATA: String     = \"data/cust.csv\"\n",
    "val CLICKS_DATA: String       = \"data/clicks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "music_schema = StructType(StructField(event_id,IntegerType,true), StructField(customer_id,IntegerType,true), StructField(track_id,StringType,true), StructField(datetime,StringType,true), StructField(is_mobile,IntegerType,true), StructField(zip,IntegerType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(event_id,IntegerType,true), StructField(customer_id,IntegerType,true), StructField(track_id,StringType,true), StructField(datetime,StringType,true), StructField(is_mobile,IntegerType,true), StructField(zip,IntegerType,true))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//define the schema, corresponding to a line in the csv data file for music\n",
    "val music_schema: StructType = new StructType(\n",
    "                                    Array(\n",
    "                                        new StructField(\"event_id\", IntegerType, nullable=true),\n",
    "                                        new StructField(\"customer_id\", IntegerType, nullable=true),\n",
    "                                        new StructField(\"track_id\", StringType, nullable=true),\n",
    "                                        new StructField(\"datetime\", StringType, nullable=true),\n",
    "                                        new StructField(\"is_mobile\", IntegerType, nullable=true),\n",
    "                                        new StructField(\"zip\", IntegerType, nullable=true)\n",
    "                                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cust_schema = StructType(StructField(customer_id,IntegerType,true), StructField(name,StringType,true), StructField(gender,IntegerType,true), StructField(address,StringType,true), StructField(zip,IntegerType,true), StructField(sign_date,StringType,true), StructField(status,IntegerType,true), StructField(level,IntegerType,true), StructField(campaign,IntegerType,true), StructField(lnkd_with_apps,IntegerType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(customer_id,IntegerType,true), StructField(name,StringType,true), StructField(gender,IntegerType,true), StructField(address,StringType,true), StructField(zip,IntegerType,true), StructField(sign_date,StringType,true), StructField(status,IntegerType,true), StructField(level,IntegerType,true), StructField(campaign,IntegerType,true), StructField(lnkd_with_apps,IntegerType,true))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//define the schema, corresponding to a line in the csv data file for customer\n",
    "val cust_schema: StructType = new StructType(\n",
    "                                    Array(\n",
    "                                        new StructField(\"customer_id\", IntegerType, nullable=true),\n",
    "                                        new StructField(\"name\", StringType, nullable=true),\n",
    "                                        new StructField(\"gender\", IntegerType, nullable=true),\n",
    "                                        new StructField(\"address\", StringType, nullable=true),\n",
    "                                        new StructField(\"zip\", IntegerType, nullable=true),\n",
    "                                        new StructField(\"sign_date\", StringType, nullable=true),\n",
    "                                        new StructField(\"status\", IntegerType, nullable=true),\n",
    "                                        new StructField(\"level\", IntegerType, nullable=true),\n",
    "                                        new StructField(\"campaign\", IntegerType, nullable=true),\n",
    "                                        new StructField(\"lnkd_with_apps\", IntegerType, nullable=true)\n",
    "                                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "click_schema = StructType(StructField(event_id,IntegerType,true), StructField(customer_id,IntegerType,true), StructField(ad_clicked,StringType,true), StructField(datetime,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(event_id,IntegerType,true), StructField(customer_id,IntegerType,true), StructField(ad_clicked,StringType,true), StructField(datetime,StringType,true))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//define the schema, corresponding to a line in the csv data file for ad click\n",
    "val click_schema: StructType = StructType(\n",
    "                                  Array(\n",
    "                                    new StructField(\"event_id\", IntegerType, nullable=true),\n",
    "                                    new StructField(\"customer_id\", IntegerType, nullable=true),\n",
    "                                    new StructField(\"ad_clicked\", StringType, nullable=true),\n",
    "                                    new StructField(\"datetime\", StringType, nullable=true)\n",
    "                                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "music_df = [event_id: int, customer_id: int ... 4 more fields]\n",
       "cust_df = [customer_id: int, name: string ... 8 more fields]\n",
       "click_df = [event_id: int, customer_id: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: int, customer_id: int ... 2 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Load data\n",
    "val music_df: DataFrame = spark.read.schema(music_schema).csv(path=MUSIC_TRACKS_DATA).cache()\n",
    "music_df.createOrReplaceTempView(\"music\")\n",
    "\n",
    "val cust_df: DataFrame = spark.read.schema(cust_schema).option(\"header\", \"true\").csv(path=CUSTOMER_DATA).cache()\n",
    "cust_df.createOrReplaceTempView(\"cust\")\n",
    "\n",
    "val click_df: DataFrame = spark.read.schema(click_schema).option(\"header\", \"false\").csv(path=CLICKS_DATA).cache()\n",
    "click_df.createOrReplaceTempView(\"click\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "//How many music data rows\n",
    "println(music_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+-------------------+---------+-----+\n",
      "|event_id|customer_id|track_id|           datetime|is_mobile|  zip|\n",
      "+--------+-----------+--------+-------------------+---------+-----+\n",
      "|       0|         48|     453|2014-10-23 03:26:20|        0|72132|\n",
      "|       1|       1081|      19|2014-10-15 18:32:14|        1|17307|\n",
      "|       2|        532|      36|2014-12-10 15:33:16|        1|66216|\n",
      "|       3|       2641|     822|2014-10-20 02:24:55|        1|36690|\n",
      "|       4|       2251|     338|2014-11-18 07:16:05|        1|61377|\n",
      "+--------+-----------+--------+-------------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "music_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "//How many customer data rows\n",
    "println(cust_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+--------------------+-----+----------+------+-----+--------+--------------+\n",
      "|customer_id|         name|gender|             address|  zip| sign_date|status|level|campaign|lnkd_with_apps|\n",
      "+-----------+-------------+------+--------------------+-----+----------+------+-----+--------+--------------+\n",
      "|          0|Gregory Koval|     0|13004 Easy Cider ...|72132|06/04/2013|     1|    1|       1|             0|\n",
      "|          1|Robert Gordon|     0|10497 Thunder Hic...|17307|07/27/2013|     1|    1|       1|             0|\n",
      "|          2|Paula Peltier|     0|10084 Easy Gate Bend|66216|01/13/2013|     1|    0|       4|             1|\n",
      "|          3|Francine Gray|     0|54845 Bent Pony H...|36690|07/11/2013|     1|    1|       1|             1|\n",
      "|          4| David Garcia|     0|8551 Tawny Fox Villa|61377|09/09/2012|     1|    0|       1|             1|\n",
      "+-----------+-------------+------+--------------------+-----+----------+------+-----+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65711\n"
     ]
    }
   ],
   "source": [
    "//How many ads click data rows\n",
    "println(click_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+-------------------+\n",
      "|event_id|customer_id|          ad_clicked|           datetime|\n",
      "+--------+-----------+--------------------+-------------------+\n",
      "|   76611|       2488|   ADV_FREE_REFERRAL|2014-12-25 05:08:59|\n",
      "|  305706|       2476|ADV_DONATION_CHARITY|2014-11-26 22:24:21|\n",
      "|  156074|       1307|   ADV_FREE_REFERRAL|2014-10-15 03:52:40|\n",
      "|  192762|       1733|   ADV_LIKE_FACEBOOK|2014-10-20 14:55:08|\n",
      "|   76106|          2|   ADV_LIKE_FACEBOOK|2014-11-19 00:22:13|\n",
      "+--------+-----------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "click_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Compute Hourly Summary profile of each customer:\n",
    "\n",
    "We will now see customers' listening behaviour across various hours in the day. Whether they tend to listen more in the morning or night, statistics like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a new Hour Column to the Music data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hourly_music_df = [event_id: int, customer_id: int ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: int, customer_id: int ... 5 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var hourly_music_df: DataFrame = music_df.withColumn(\"hour\", hour(col(\"datetime\"))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+-------------------+---------+-----+----+\n",
      "|event_id|customer_id|track_id|           datetime|is_mobile|  zip|hour|\n",
      "+--------+-----------+--------+-------------------+---------+-----+----+\n",
      "|       0|         48|     453|2014-10-23 03:26:20|        0|72132|   3|\n",
      "|       1|       1081|      19|2014-10-15 18:32:14|        1|17307|  18|\n",
      "|       2|        532|      36|2014-12-10 15:33:16|        1|66216|  15|\n",
      "|       3|       2641|     822|2014-10-20 02:24:55|        1|36690|   2|\n",
      "|       4|       2251|     338|2014-11-18 07:16:05|        1|61377|   7|\n",
      "+--------+-----------+--------+-------------------+---------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hourly_music_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divide the entire day into four time buckets based on the hour:**  \n",
    "\n",
    "Bucket the listen datetime into different buckets in the day e.g. night, morning, afternoon or evening and mark 1 if the song is listened in that bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hourly_music_df = [event_id: int, customer_id: int ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: int, customer_id: int ... 9 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_music_df = (hourly_music_df\n",
    "      .withColumn(\"night\", when((col(\"hour\") < 5) || (col(\"hour\") >= 22), 1).otherwise(0))\n",
    "      .withColumn(\"morn\", when((col(\"hour\") >= 5) && (col(\"hour\") < 12), 1).otherwise(0))\n",
    "      .withColumn(\"aft\", when((col(\"hour\") >= 12) && (col(\"hour\") < 17), 1).otherwise(0))\n",
    "      .withColumn(\"eve\", when((col(\"hour\") >= 17) && (col(\"hour\") < 22), 1).otherwise(0))\n",
    "      .cache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Compute Customer Hourly Summary using DF API:\n",
    "\n",
    "Now we're ready to compute a summary profile for each user. We will leverage Spark SQL functions compute some high-level data:\n",
    "\n",
    "+ Average number of tracks listened during each period of the day: morning, afternoon, evening, and night. We arbitrarily define the time ranges in the code.\n",
    "+ Total unique tracks listened by that user, i.e. the set of unique track IDs.\n",
    "+ Total mobile tracks listened by that user, i.e. the count of tracks that were listened that had their mobile flag set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cust_profile_df = [customer_id: int, count(DISTINCT track_id): bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: int, count(DISTINCT track_id): bigint ... 5 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cust_profile_df: DataFrame = (hourly_music_df\n",
    "                                  .select(\"customer_id\", \"track_id\", \"night\", \"morn\", \"aft\", \"eve\", \"is_mobile\")\n",
    "                                  .groupBy(\"customer_id\")\n",
    "                                  .agg(countDistinct(\"track_id\"), sum(\"night\"),sum(\"morn\"),sum(\"aft\"),sum(\"eve\"), sum(\"is_mobile\")\n",
    "                                  )).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|customer_id|count(DISTINCT track_id)|sum(night)|sum(morn)|sum(aft)|sum(eve)|sum(is_mobile)|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|        148|                     443|       149|      170|     109|     124|           476|\n",
      "|        463|                     306|       103|       99|      84|      76|           176|\n",
      "|       1591|                     171|        47|       64|      36|      40|            85|\n",
      "|       2366|                     143|        55|       46|      30|      25|           113|\n",
      "|       4101|                     100|        31|       28|      26|      22|            85|\n",
      "|       1342|                     173|        53|       60|      36|      42|           102|\n",
      "|       2659|                     119|        42|       43|      22|      22|            59|\n",
      "|       1238|                     191|        72|       64|      30|      46|           158|\n",
      "|       4519|                     103|        37|       30|      20|      20|            54|\n",
      "|       1580|                     162|        44|       52|      43|      41|           134|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_profile_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Compute Customer Hourly Summary using SQL:\n",
    "\n",
    "In the previous sections we used only DF APIs to calculate the hourly profiles. However, we can use pure Spark SQL to achieve the same results. That would be much less verbose. We will still leverage PySpark SQL functions compute those high-level data:\n",
    "\n",
    "+ Average number of tracks listened during each period of the day: morning, afternoon, evening, and night. We arbitrarily define the time ranges in the code.\n",
    "+ Total unique tracks listened by that user, i.e. the set of unique track IDs.\n",
    "+ Total mobile tracks listened by that user, i.e. the count of tracks that were listened that had their mobile flag set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divide the entire day into four time buckets based on the hour:**  \n",
    "\n",
    "Bucket the listen datetime into different buckets in the day e.g. night, morning, afternoon or evening and mark 1 if the song is listened in that bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+-------------------+---------+-----+----+-----+----+---+---+\n",
      "|event_id|customer_id|track_id|           datetime|is_mobile|  zip|hour|night|morn|aft|eve|\n",
      "+--------+-----------+--------+-------------------+---------+-----+----+-----+----+---+---+\n",
      "|       0|         48|     453|2014-10-23 03:26:20|        0|72132|   3|    1|   0|  0|  0|\n",
      "|       1|       1081|      19|2014-10-15 18:32:14|        1|17307|  18|    0|   0|  0|  1|\n",
      "|       2|        532|      36|2014-12-10 15:33:16|        1|66216|  15|    0|   0|  1|  0|\n",
      "|       3|       2641|     822|2014-10-20 02:24:55|        1|36690|   2|    1|   0|  0|  0|\n",
      "|       4|       2251|     338|2014-11-18 07:16:05|        1|61377|   7|    0|   1|  0|  0|\n",
      "|       5|       1811|       6|2014-11-18 02:00:48|        1|20115|   2|    1|   0|  0|  0|\n",
      "|       6|       3644|      24|2014-12-12 15:24:02|        1|15330|  15|    0|   0|  1|  0|\n",
      "|       7|        250|     726|2014-10-07 09:48:53|        0|33570|   9|    0|   1|  0|  0|\n",
      "|       8|       1782|     442|2014-12-30 15:27:31|        1|41240|  15|    0|   0|  1|  0|\n",
      "|       9|       2932|     775|2014-11-12 07:45:55|        0|63565|   7|    0|   1|  0|  0|\n",
      "+--------+-----------+--------+-------------------+---------+-----+----+-----+----+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT  *,\n",
    "      HOUR(datetime) as hour,\n",
    "      CASE WHEN HOUR(datetime) < 5 OR HOUR(datetime) >= 22 THEN 1 ELSE 0 END AS night,\n",
    "      CASE WHEN HOUR(datetime) >= 5 AND HOUR(datetime) < 12 THEN 1 ELSE 0 END AS morn,\n",
    "      CASE WHEN HOUR(datetime) >= 12 AND HOUR(datetime) < 17 THEN 1 ELSE 0 END AS aft,\n",
    "      CASE WHEN HOUR(datetime) >= 17 AND HOUR(datetime) < 22 THEN 1 ELSE 0 END AS eve\n",
    "FROM music\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the hourly profiles:**  \n",
    "\n",
    "We can combine the above bucketing and calculating the hourly summary in one SQL as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|customer_id|count(DISTINCT track_id)|sum(night)|sum(morn)|sum(aft)|sum(eve)|sum(is_mobile)|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|        148|                     443|       149|      170|     109|     124|           476|\n",
      "|        463|                     306|       103|       99|      84|      76|           176|\n",
      "|       1591|                     171|        47|       64|      36|      40|            85|\n",
      "|       2366|                     143|        55|       46|      30|      25|           113|\n",
      "|       4101|                     100|        31|       28|      26|      22|            85|\n",
      "|       1342|                     173|        53|       60|      36|      42|           102|\n",
      "|       2659|                     119|        42|       43|      22|      22|            59|\n",
      "|       1238|                     191|        72|       64|      30|      46|           158|\n",
      "|       4519|                     103|        37|       30|      20|      20|            54|\n",
      "|       1580|                     162|        44|       52|      43|      41|           134|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT customer_id, COUNT(DISTINCT track_id), SUM(night), SUM(morn), SUM(aft), SUM(eve), SUM(is_mobile)\n",
    "FROM(\n",
    "  SELECT  *,\n",
    "          HOUR(datetime) as hour,\n",
    "          CASE WHEN HOUR(datetime) < 5 OR HOUR(datetime) >= 22 THEN 1 ELSE 0 END AS night,\n",
    "          CASE WHEN HOUR(datetime) >= 5 AND HOUR(datetime) < 12 THEN 1 ELSE 0 END AS morn,\n",
    "          CASE WHEN HOUR(datetime) >= 12 AND HOUR(datetime) < 17 THEN 1 ELSE 0 END AS aft,\n",
    "          CASE WHEN HOUR(datetime) >= 17 AND HOUR(datetime) < 22 THEN 1 ELSE 0 END AS eve\n",
    "  FROM music)\n",
    "GROUP BY customer_id\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the result is same as the results form the DF APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Summary Statistics:\n",
    "\n",
    "Since we have the summary data readily available we compute some basic statistics on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+-----------------+----------------+-----------------+-----------------+------------------+\n",
      "|summary|count(DISTINCT track_id)|       sum(night)|       sum(morn)|         sum(aft)|         sum(eve)|    sum(is_mobile)|\n",
      "+-------+------------------------+-----------------+----------------+-----------------+-----------------+------------------+\n",
      "|  count|                    5000|             5000|            5000|             5000|             5000|              5000|\n",
      "|   mean|                 170.295|          58.3032|         58.2908|          41.6434|          41.7626|           121.553|\n",
      "| stddev|      117.04437556828793|67.27232404842705|67.3964412370437|47.87538247251274|48.01370329792189|148.79537090743347|\n",
      "|    min|                      68|               15|              16|                9|                9|                32|\n",
      "|    max|                    1617|             2139|            2007|             1460|             1480|              5093|\n",
      "+-------+------------------------+-----------------+----------------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Referring to cust_profile_df from section 4.1.1 we can use the describe() function to get the summary statistics\n",
    "cust_profile_df.select(cust_profile_df.columns.filter(c => !c.equals(\"customer_id\")).map(col): _*).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary_stats_df = [summary: string, count(DISTINCT track_id): string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, count(DISTINCT track_id): string ... 5 more fields]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// store the describe dataframe temporarily\n",
    "var summary_stats_df: DataFrame = cust_profile_df.select(cust_profile_df.columns.filter(c => !c.equals(\"customer_id\")).map(col): _*).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Prettifying Summary Statistics:\n",
    "\n",
    "There are too many decimal places for mean and stddev in the describe() dataframe. We can format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that describe() returns, its a String, we need to cast that to a float before we can format. We use cast() and format_number() on individual columns to reformat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+----------+---------+--------+--------------+\n",
      "|summary|count(DISTINCT track_id)|sum(night)|sum(morn)|sum(aft)|sum(is_mobile)|\n",
      "+-------+------------------------+----------+---------+--------+--------------+\n",
      "|  count|                5,000.00|  5,000.00| 5,000.00|5,000.00|      5,000.00|\n",
      "|   mean|                  170.29|     58.30|    58.29|   41.64|        121.55|\n",
      "| stddev|                  117.04|     67.27|    67.40|   47.88|        148.80|\n",
      "|    min|                   68.00|     15.00|    16.00|    9.00|         32.00|\n",
      "|    max|                1,617.00|  2,139.00| 2,007.00|1,460.00|      5,093.00|\n",
      "+-------+------------------------+----------+---------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_stats_df.select(summary_stats_df(\"summary\"),\n",
    "  format_number(summary_stats_df(\"count(DISTINCT track_id)\").cast(\"float\"), 2).alias(\"count(DISTINCT track_id)\"),\n",
    "  format_number(summary_stats_df(\"sum(night)\").cast(\"float\"), 2).alias(\"sum(night)\"),\n",
    "  format_number(summary_stats_df(\"sum(morn)\").cast(\"float\"), 2).alias(\"sum(morn)\"),\n",
    "  format_number(summary_stats_df(\"sum(aft)\").cast(\"float\"), 2).alias(\"sum(aft)\"),\n",
    "  format_number(summary_stats_df(\"sum(is_mobile)\").cast(\"float\"), 2).alias(\"sum(is_mobile)\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Prettifying Summary Statistics - Even Smarter:\n",
    "\n",
    "In real life data sets there would be too many columns. Specifying each columm in the codes would not be feasible. We can use list comprehension of Python of for loops to do this smartly. We can even exclude some columns we dont' want.\n",
    "\n",
    "**Apply for loop on formatting columns and excluding the summary column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for(col_name <- summary_stats_df.columns.filter(col_name => !col_name.equals(\"summary\"))) {\n",
    "  summary_stats_df = summary_stats_df.withColumn(col_name, format_number(col(col_name).cast(\"float\"), 2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|summary|count(DISTINCT track_id)|sum(night)|sum(morn)|sum(aft)|sum(eve)|sum(is_mobile)|\n",
      "+-------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|  count|                5,000.00|  5,000.00| 5,000.00|5,000.00|5,000.00|      5,000.00|\n",
      "|   mean|                  170.29|     58.30|    58.29|   41.64|   41.76|        121.55|\n",
      "| stddev|                  117.04|     67.27|    67.40|   47.88|   48.01|        148.80|\n",
      "|    min|                   68.00|     15.00|    16.00|    9.00|    9.00|         32.00|\n",
      "|    max|                1,617.00|  2,139.00| 2,007.00|1,460.00|1,480.00|      5,093.00|\n",
      "+-------+------------------------+----------+---------+--------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_stats_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the summary statistics:\n",
    "> People Listen to highest number of songs in the Night!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 An ODE to RDD - Compute Customer Hourly Summary using Custom Group Function:\n",
    "\n",
    "If you and must have to work with RDD instead of DataFrames, then we can compute a summary profile for each user by passing a function we'll write to mapValues to compute the same high-level data:\n",
    "\n",
    "+ Average number of tracks listened during each period of the day: morning, afternoon, evening, and night. We arbitrarily define the time ranges in the code.\n",
    "+ Total unique tracks listened by that user, i.e. the set of unique track IDs.\n",
    "+ Total mobile tracks listened by that user, i.e. the count of tracks that were listened that had their mobile flag set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "music_rdd = MapPartitionsRDD[117] at rdd at <console>:42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[117] at rdd at <console>:42"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//let's select only the original columns\n",
    "val music_rdd: RDD[Row] = music_df.select(\"customer_id\", \"track_id\", \"datetime\", \"is_mobile\", \"zip\").rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48,453,2014-10-23 03:26:20,0,72132]\n",
      "[1081,19,2014-10-15 18:32:14,1,17307]\n",
      "[532,36,2014-12-10 15:33:16,1,66216]\n",
      "[2641,822,2014-10-20 02:24:55,1,36690]\n",
      "[2251,338,2014-11-18 07:16:05,1,61377]\n"
     ]
    }
   ],
   "source": [
    "music_rdd.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use customer_id as the key:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48,[48,453,2014-10-23 03:26:20,0,72132])\n",
      "(1081,[1081,19,2014-10-15 18:32:14,1,17307])\n",
      "(532,[532,36,2014-12-10 15:33:16,1,66216])\n",
      "(2641,[2641,822,2014-10-20 02:24:55,1,36690])\n",
      "(2251,[2251,338,2014-11-18 07:16:05,1,61377])\n"
     ]
    }
   ],
   "source": [
    "//Use customer_id as the key, we will later group by on this column\n",
    "music_rdd.map(record => (record(0), record)).take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Develop the User Stats function:**\n",
    "\n",
    "We loop over the tracks of each customer and find the unique number of tracks listened by him and how many times he listened during various times of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_stats_byuser: (tracks: Iterable[org.apache.spark.sql.Row])(Double, Double, Double, Double, Double, Double)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_stats_byuser(tracks: Iterable[Row]) : (Double, Double, Double, Double, Double, Double) = {\n",
    "\n",
    "  var mcount, morn, aft, eve, night = 0\n",
    "  val tracklist: Set[String] = Set()\n",
    "\n",
    "  for(track <- tracks) {\n",
    "    //println(track)\n",
    "    //println(track.schema)\n",
    "\n",
    "    val custid = track.getAs[Int](0)\n",
    "    val trackid = track.getAs[String](1)\n",
    "    val hour = track.getAs[String](2).split(\" \")(1).split(\":\")(0).toInt\n",
    "    val mobile = track.getAs[Int](3)\n",
    "    val zip = track.getAs[Int](4)\n",
    "\n",
    "    tracklist.add(trackid)\n",
    "\n",
    "    mcount += mobile\n",
    "\n",
    "    if (hour < 5) {\n",
    "      night += 1\n",
    "    } else if (hour < 12) {\n",
    "      morn += 1\n",
    "    } else if (hour < 17) {\n",
    "      aft += 1\n",
    "    } else if (hour < 22) {\n",
    "      eve += 1\n",
    "    } else {\n",
    "      night += 1\n",
    "    }\n",
    "\n",
    "  }\n",
    "\n",
    "  (tracklist.size, morn, aft, eve, night, mcount)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cust_profile_rdd = MapPartitionsRDD[121] at mapValues at <console>:46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[121] at mapValues at <console>:46"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cust_profile_rdd = (music_rdd.map(record => (record(0), record))\n",
    "                                .groupByKey().mapValues(tracks => compute_stats_byuser(tracks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[121] at mapValues at <console>:46"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_profile_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4904,(106.0,40.0,20.0,20.0,33.0,72.0))\n",
      "(4552,(105.0,24.0,22.0,33.0,32.0,91.0))\n",
      "(3456,(96.0,29.0,23.0,25.0,23.0,47.0))\n",
      "(4680,(91.0,25.0,21.0,22.0,29.0,49.0))\n",
      "(1080,(185.0,71.0,49.0,29.0,58.0,98.0))\n",
      "(320,(313.0,108.0,68.0,92.0,124.0,328.0))\n",
      "(752,(260.0,87.0,70.0,66.0,78.0,158.0))\n",
      "(3272,(112.0,38.0,20.0,27.0,35.0,53.0))\n",
      "(408,(272.0,101.0,71.0,58.0,91.0,231.0))\n",
      "(4352,(104.0,33.0,26.0,24.0,31.0,86.0))\n"
     ]
    }
   ],
   "source": [
    "cust_profile_rdd.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the Results that we got from RDD and previously from DF methods:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(48,(696.0,310.0,217.0,223.0,277.0,503.0))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_profile_rdd.filter(record => record._1 == 48).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|customer_id|count(DISTINCT track_id)|sum(night)|sum(morn)|sum(aft)|sum(eve)|sum(is_mobile)|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|         48|                     696|       277|      310|     217|     223|           503|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_profile_df.filter(col(\"customer_id\") === 48).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo Hoo! We can clearly see that the values in each of the columns are matching! We are on the right track!\n",
    "\n",
    "\n",
    "**Summary Statistics:**\n",
    "\n",
    "Since we have the summary data readily available we compute some basic statistics on it. Since we are working on the RDD we cannot use the `describe()` method of the DataFrame. Instead we will use the `Statistics` package  for the `colStats` function from `org.apache.spark.mllib.stat.Statistics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary_stats_ml = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@598e2a85\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@598e2a85"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.stat.Statistics\n",
    "\n",
    "//compute aggregate stats for entire track history\n",
    "val summary_stats_ml = Statistics.colStats(cust_profile_rdd.map(x => Vectors.dense(Array(x._2._1, x._2._2, x._2._3, x._2._4, x._2._5, x._2._6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "println(summary_stats_ml.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170.29499999999996,58.29079999999999,41.64339999999999,41.762599999999985,58.3032,121.55300000000003]\n"
     ]
    }
   ],
   "source": [
    "println(summary_stats_ml.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1617.0,2007.0,1460.0,1480.0,2139.0,5093.0]\n"
     ]
    }
   ],
   "source": [
    "println(summary_stats_ml.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68.0,16.0,9.0,9.0,15.0,32.0]\n"
     ]
    }
   ],
   "source": [
    "println(summary_stats_ml.min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 PIVOT Tables With Multiples WHENs - Compute Customer Hourly Summary:\n",
    "\n",
    "If you intend to venture on using more advanced functions in Spark, then we can use the `pivot` function to do whate we have done doe now in much shorter steps.\n",
    "\n",
    "First we extract the hour, convert that hour into several buckets and then pivot on those buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+-------------------+---------+-----+----+\n",
      "|event_id|customer_id|track_id|           datetime|is_mobile|  zip|hour|\n",
      "+--------+-----------+--------+-------------------+---------+-----+----+\n",
      "|       0|         48|     453|2014-10-23 03:26:20|        0|72132|   3|\n",
      "|       1|       1081|      19|2014-10-15 18:32:14|        1|17307|  18|\n",
      "|       2|        532|      36|2014-12-10 15:33:16|        1|66216|  15|\n",
      "|       3|       2641|     822|2014-10-20 02:24:55|        1|36690|   2|\n",
      "|       4|       2251|     338|2014-11-18 07:16:05|        1|61377|   7|\n",
      "|       5|       1811|       6|2014-11-18 02:00:48|        1|20115|   2|\n",
      "|       6|       3644|      24|2014-12-12 15:24:02|        1|15330|  15|\n",
      "|       7|        250|     726|2014-10-07 09:48:53|        0|33570|   9|\n",
      "|       8|       1782|     442|2014-12-30 15:27:31|        1|41240|  15|\n",
      "|       9|       2932|     775|2014-11-12 07:45:55|        0|63565|   7|\n",
      "+--------+-----------+--------+-------------------+---------+-----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "music_df.select(\n",
    "  col(\"event_id\"),\n",
    "  col(\"customer_id\"),\n",
    "  col(\"track_id\"),\n",
    "  col(\"datetime\"),\n",
    "  col(\"is_mobile\"),\n",
    "  col(\"zip\"),\n",
    "  hour(col(\"datetime\")).alias(\"hour\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+-------------------+---------+-----+----+------+\n",
      "|event_id|customer_id|track_id|           datetime|is_mobile|  zip|hour|bucket|\n",
      "+--------+-----------+--------+-------------------+---------+-----+----+------+\n",
      "|       0|         48|     453|2014-10-23 03:26:20|        0|72132|   3| night|\n",
      "|       1|       1081|      19|2014-10-15 18:32:14|        1|17307|  18|   eve|\n",
      "|       2|        532|      36|2014-12-10 15:33:16|        1|66216|  15|   aft|\n",
      "|       3|       2641|     822|2014-10-20 02:24:55|        1|36690|   2| night|\n",
      "|       4|       2251|     338|2014-11-18 07:16:05|        1|61377|   7|  morn|\n",
      "|       5|       1811|       6|2014-11-18 02:00:48|        1|20115|   2| night|\n",
      "|       6|       3644|      24|2014-12-12 15:24:02|        1|15330|  15|   aft|\n",
      "|       7|        250|     726|2014-10-07 09:48:53|        0|33570|   9|  morn|\n",
      "|       8|       1782|     442|2014-12-30 15:27:31|        1|41240|  15|   aft|\n",
      "|       9|       2932|     775|2014-11-12 07:45:55|        0|63565|   7|  morn|\n",
      "+--------+-----------+--------+-------------------+---------+-----+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Create the hour buckets\n",
    "music_df\n",
    "  .select(col(\"event_id\"), col(\"customer_id\"), col(\"track_id\"), col(\"datetime\"), col(\"is_mobile\"), col(\"zip\"),\n",
    "   hour(col(\"datetime\")).alias(\"hour\"),\n",
    "   when((hour(col(\"datetime\")) < 5) || (hour(col(\"datetime\")) >= 22), lit(\"night\"))\n",
    "   .when((hour(col(\"datetime\")) >= 5) && (hour(col(\"datetime\")) < 12), lit(\"morn\"))\n",
    "   .when((hour(col(\"datetime\")) >= 12) && (hour(col(\"datetime\")) < 17), lit(\"aft\"))\n",
    "   .when((hour(col(\"datetime\")) >= 17) && (hour(col(\"datetime\")) < 22), lit(\"eve\"))\n",
    "   .alias(\"bucket\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hourly_pivot_df = [customer_id: int, night: bigint ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: int, night: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create the hour buckets and then pivot on the hour buckets\n",
    "val hourly_pivot_df = music_df\n",
    "                        .select(col(\"event_id\"), col(\"customer_id\"), col(\"track_id\"), col(\"datetime\"), \n",
    "                                col(\"is_mobile\"), col(\"zip\"), hour(col(\"datetime\")).alias(\"hour\"),\n",
    "                          when((hour(col(\"datetime\")) < 5) || (hour(col(\"datetime\")) >= 22), lit(\"night\"))\n",
    "                           .when((hour(col(\"datetime\")) >= 5) && (hour(col(\"datetime\")) < 12), lit(\"morn\"))\n",
    "                           .when((hour(col(\"datetime\")) >= 12) && (hour(col(\"datetime\")) < 17), lit(\"aft\"))\n",
    "                           .when((hour(col(\"datetime\")) >= 17) && (hour(col(\"datetime\")) < 22), lit(\"eve\"))\n",
    "                           .alias(\"bucket\"))\n",
    "                        .select(\"customer_id\", \"bucket\")\n",
    "                        .groupBy(\"customer_id\")\n",
    "                        .pivot(\"bucket\", Array(\"night\", \"morn\", \"aft\", \"eve\"))\n",
    "                        .agg(count(col(\"bucket\"))\n",
    "                        ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----+---+---+\n",
      "|customer_id|night|morn|aft|eve|\n",
      "+-----------+-----+----+---+---+\n",
      "|        471|   84|  96| 60| 73|\n",
      "|       3175|   35|  28| 25| 21|\n",
      "|        833|   70|  75| 48| 63|\n",
      "|       1088|   69|  62| 41| 46|\n",
      "|        463|  103|  99| 84| 76|\n",
      "|       1238|   72|  64| 30| 46|\n",
      "|       1645|   55|  42| 54| 35|\n",
      "|       1342|   53|  60| 36| 42|\n",
      "|       1959|   42|  43| 34| 24|\n",
      "|       2366|   55|  46| 30| 25|\n",
      "+-----------+-----+----+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hourly_pivot_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the Profile Summary that we got from Multi Step DF API and SQL above and the Pivot operation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----+---+---+\n",
      "|customer_id|night|morn|aft|eve|\n",
      "+-----------+-----+----+---+---+\n",
      "|         48|  277| 310|217|223|\n",
      "+-----------+-----+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hourly_pivot_df.filter(col(\"customer_id\") === 48).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|customer_id|count(DISTINCT track_id)|sum(night)|sum(morn)|sum(aft)|sum(eve)|sum(is_mobile)|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|         48|                     696|       277|      310|     217|     223|           503|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_profile_df.filter(col(\"customer_id\") === 48).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YAY!** We can clearly see that the results from our pivot operation and the results we got from DF API and SQL constructs are matching! To make it exactly we would need give a final touch!\n",
    "\n",
    "Gather the stats for no. of unique tracks and is_mobile count separately and then join with the pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tracks_summary_df = [customer_id: int, count(DISTINCT track_id): bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: int, count(DISTINCT track_id): bigint ... 1 more field]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tracks_summary_df = (music_df\n",
    "  .select(\"customer_id\", \"track_id\", \"is_mobile\")\n",
    "  .groupBy(\"customer_id\")\n",
    "  .agg(countDistinct(\"track_id\"), sum(\"is_mobile\"))\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+--------------+\n",
      "|customer_id|count(DISTINCT track_id)|sum(is_mobile)|\n",
      "+-----------+------------------------+--------------+\n",
      "|        148|                     443|           476|\n",
      "|        463|                     306|           176|\n",
      "|       1591|                     171|            85|\n",
      "|       2366|                     143|           113|\n",
      "|       4101|                     100|            85|\n",
      "|       1342|                     173|           102|\n",
      "|       2659|                     119|            59|\n",
      "|       1238|                     191|           158|\n",
      "|       4519|                     103|            54|\n",
      "|       1580|                     162|           134|\n",
      "+-----------+------------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tracks_summary_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|customer_id|count(DISTINCT track_id)|sum(night)|sum(morn)|sum(aft)|sum(eve)|sum(is_mobile)|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|         48|                     696|       277|      310|     217|     223|           503|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(tracks_summary_df\n",
    "      .join(hourly_pivot_df, hourly_pivot_df(\"customer_id\") === tracks_summary_df(\"customer_id\"), \"inner\")\n",
    "      .select(hourly_pivot_df(\"customer_id\"), col(\"count(DISTINCT track_id)\"),\n",
    "      col(\"night\").alias(\"sum(night)\"),\n",
    "      col(\"morn\").alias(\"sum(morn)\"),\n",
    "      col(\"aft\").alias(\"sum(aft)\"),\n",
    "      col(\"eve\").alias(\"sum(eve)\"),\n",
    "        col(\"sum(is_mobile)\"))\n",
    "      .filter(col(\"customer_id\") === 48)\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|customer_id|count(DISTINCT track_id)|sum(night)|sum(morn)|sum(aft)|sum(eve)|sum(is_mobile)|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "|         48|                     696|       277|      310|     217|     223|           503|\n",
      "+-----------+------------------------+----------+---------+--------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_profile_df.filter(col(\"customer_id\") === 48).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[customer_id: int, count(DISTINCT track_id): bigint ... 1 more field]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_pivot_df.unpersist()\n",
    "tracks_summary_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 PIVOT & UNPIVOT Tables:\n",
    "\n",
    "Often times we would need to UNPIVOT tables. This is just the reverse of PIVOT function, converting from a wide format to narrow format. It is similar to pandas `melt` function.\n",
    "\n",
    "We can realise that through a combination of tsruct `explode(array({struct(<col name>,<col val>)}*))` transformations.\n",
    "\n",
    "First we extract the hour, convert that hour into several buckets and then pivot on those buckets to create the hourly_pivot DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hourly_pivot_df = [customer_id: int, night: bigint ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: int, night: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create the hour buckets and then pivot on the hour buckets\n",
    "val hourly_pivot_df = (music_df.select($\"event_id\", $\"customer_id\", $\"track_id\", $\"datetime\", $\"is_mobile\", $\"zip\", \n",
    "             hour($\"datetime\").alias(\"hour\"),\n",
    "             when((hour($\"datetime\") < 5) or (hour($\"datetime\") >= 22), lit(\"night\"))\n",
    "              .when((hour($\"datetime\") >= 5) and (hour($\"datetime\") < 12), lit(\"morn\"))\n",
    "              .when((hour($\"datetime\") >= 12) and (hour($\"datetime\") < 17), lit(\"aft\"))\n",
    "              .when((hour($\"datetime\") >= 17) and (hour($\"datetime\") < 22), lit(\"eve\"))\n",
    "              .alias(\"bucket\"))\n",
    "              .select(\"customer_id\", \"bucket\")\n",
    "              .groupBy(\"customer_id\")\n",
    "              .pivot(\"bucket\", Array(\"night\", \"morn\", \"aft\", \"eve\"))\n",
    "              .agg(count($\"bucket\"))\n",
    "           ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----+---+---+\n",
      "|customer_id|night|morn|aft|eve|\n",
      "+-----------+-----+----+---+---+\n",
      "|        471|   84|  96| 60| 73|\n",
      "|       3175|   35|  28| 25| 21|\n",
      "|        833|   70|  75| 48| 63|\n",
      "|       1088|   69|  62| 41| 46|\n",
      "|        463|  103|  99| 84| 76|\n",
      "|       1238|   72|  64| 30| 46|\n",
      "|       1645|   55|  42| 54| 35|\n",
      "|       1342|   53|  60| 36| 42|\n",
      "|       1959|   42|  43| 34| 24|\n",
      "|       2366|   55|  46| 30| 25|\n",
      "+-----------+-----+----+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hourly_pivot_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert each column into a struct column and the combine all those struct columns to form an array of struct columns.  It is important to provide same names to the individual elements with the struct columns otherwise the array function will complain that it has not been provided with similar elements e.g. `struct(lit(\"night\").alias(\"bucket\"), col(\"night\").alias(\"count\"))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------------------+\n",
      "|customer_id|array_of_struct_bucket_count                    |\n",
      "+-----------+------------------------------------------------+\n",
      "|471        |[[night, 84], [morn, 96], [aft, 60], [eve, 73]] |\n",
      "|3175       |[[night, 35], [morn, 28], [aft, 25], [eve, 21]] |\n",
      "|833        |[[night, 70], [morn, 75], [aft, 48], [eve, 63]] |\n",
      "|1088       |[[night, 69], [morn, 62], [aft, 41], [eve, 46]] |\n",
      "|463        |[[night, 103], [morn, 99], [aft, 84], [eve, 76]]|\n",
      "|1238       |[[night, 72], [morn, 64], [aft, 30], [eve, 46]] |\n",
      "|1645       |[[night, 55], [morn, 42], [aft, 54], [eve, 35]] |\n",
      "|1342       |[[night, 53], [morn, 60], [aft, 36], [eve, 42]] |\n",
      "|1959       |[[night, 42], [morn, 43], [aft, 34], [eve, 24]] |\n",
      "|2366       |[[night, 55], [morn, 46], [aft, 30], [eve, 25]] |\n",
      "+-----------+------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(hourly_pivot_df\n",
    "    .select($\"customer_id\", \n",
    "        array(\n",
    "            struct(lit(\"night\").alias(\"bucket\"), col(\"night\").alias(\"count\")), \n",
    "            struct(lit(\"morn\").alias(\"bucket\"), col(\"morn\").alias(\"count\")),\n",
    "            struct(lit(\"aft\").alias(\"bucket\"), col(\"aft\").alias(\"count\")),\n",
    "            struct(lit(\"eve\").alias(\"bucket\"), col(\"eve\").alias(\"count\"))\n",
    "          ).alias(\"array_of_struct_bucket_count\")\n",
    "        )).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then explode the array of structs column so that now each struct column becomes a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------+\n",
      "|customer_id|exploded_struct_bucket_count|\n",
      "+-----------+----------------------------+\n",
      "|471        |[night, 84]                 |\n",
      "|471        |[morn, 96]                  |\n",
      "|471        |[aft, 60]                   |\n",
      "|471        |[eve, 73]                   |\n",
      "|3175       |[night, 35]                 |\n",
      "|3175       |[morn, 28]                  |\n",
      "|3175       |[aft, 25]                   |\n",
      "|3175       |[eve, 21]                   |\n",
      "|833        |[night, 70]                 |\n",
      "|833        |[morn, 75]                  |\n",
      "+-----------+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(hourly_pivot_df\n",
    "    .select($\"customer_id\",    \n",
    "        explode(\n",
    "            array(\n",
    "                struct(lit(\"night\").alias(\"bucket\"), col(\"night\").alias(\"count\")), \n",
    "                struct(lit(\"morn\").alias(\"bucket\"), col(\"morn\").alias(\"count\")),\n",
    "                struct(lit(\"aft\").alias(\"bucket\"), col(\"aft\").alias(\"count\")),\n",
    "                struct(lit(\"eve\").alias(\"bucket\"), col(\"eve\").alias(\"count\"))\n",
    "            )\n",
    "          ).alias(\"exploded_struct_bucket_count\")\n",
    "        )).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finaly, We break exploded struct column into ite individual components and extract them out as separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+\n",
      "|customer_id|bucket|count|\n",
      "+-----------+------+-----+\n",
      "|471        |night |84   |\n",
      "|471        |morn  |96   |\n",
      "|3175       |night |35   |\n",
      "|3175       |morn  |28   |\n",
      "|833        |night |70   |\n",
      "|833        |morn  |75   |\n",
      "|1088       |night |69   |\n",
      "|1088       |morn  |62   |\n",
      "|463        |night |103  |\n",
      "|463        |morn  |99   |\n",
      "+-----------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(hourly_pivot_df\n",
    "    .withColumn(\"exploded_struct_bucket_count\",    \n",
    "        explode(\n",
    "            array(\n",
    "                struct(lit(\"night\").alias(\"bucket\"), col(\"night\").alias(\"count\")), \n",
    "                struct(lit(\"morn\").alias(\"bucket\"), col(\"morn\").alias(\"count\"))\n",
    "            )\n",
    "          )         \n",
    "        )\n",
    "     .selectExpr(\"customer_id\", \"exploded_struct_bucket_count.bucket as bucket\", \"exploded_struct_bucket_count.count as count\")\n",
    ").show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[customer_id: int, night: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_pivot_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Average number of tracks listened by Customers of Different Levels during Different Time of the Day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+--------------------+-----+----------+------+-----+--------+--------------+\n",
      "|customer_id|         name|gender|             address|  zip| sign_date|status|level|campaign|lnkd_with_apps|\n",
      "+-----------+-------------+------+--------------------+-----+----------+------+-----+--------+--------------+\n",
      "|          0|Gregory Koval|     0|13004 Easy Cider ...|72132|06/04/2013|     1|    1|       1|             0|\n",
      "|          1|Robert Gordon|     0|10497 Thunder Hic...|17307|07/27/2013|     1|    1|       1|             0|\n",
      "|          2|Paula Peltier|     0|10084 Easy Gate Bend|66216|01/13/2013|     1|    0|       4|             1|\n",
      "|          3|Francine Gray|     0|54845 Bent Pony H...|36690|07/11/2013|     1|    1|       1|             1|\n",
      "|          4| David Garcia|     0|8551 Tawny Fox Villa|61377|09/09/2012|     1|    0|       1|             1|\n",
      "+-----------+-------------+------+--------------------+-----+----------+------+-----+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "udfIndexTolevel = UserDefinedFunction(<function1>,StringType,None)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,StringType,None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define a udf to Map from level number to actual level string\n",
    "val udfIndexTolevel: UserDefinedFunction = udf((mon: Int) => {\n",
    "  val level_map: Map[Int, String] = Map(0 -> \"Free\", 1 -> \"Silver\", 2 -> \"Gold\")\n",
    "  level_map.get(mon)\n",
    "}, StringType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_df = [level: string, Afternoon: double ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[level: string, Afternoon: double ... 3 more fields]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var result_df: DataFrame = \n",
    "(cust_df.join(cust_profile_df, cust_df(\"customer_id\") === cust_profile_df(\"customer_id\"), \"inner\")\n",
    "  .select(udfIndexTolevel(col(\"level\")).alias(\"level\"), \n",
    "          col(\"sum(night)\"), col(\"sum(morn)\"), col(\"sum(aft)\"), col(\"sum(eve)\"))\n",
    "  .groupBy(\"level\")\n",
    "  .agg(avg(\"sum(aft)\").alias(\"Afternoon\"),\n",
    "       avg(\"sum(eve)\").alias(\"Evening\"),\n",
    "       avg(\"sum(morn)\").alias(\"Morning\"),\n",
    "       avg(\"sum(night)\").alias(\"Night\")\n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+-----------------+------------------+\n",
      "| level|         Afternoon|           Evening|          Morning|             Night|\n",
      "+------+------------------+------------------+-----------------+------------------+\n",
      "|Silver| 42.12979890310786|42.409506398537474|59.01401584399756| 59.16209628275442|\n",
      "|  Gold|39.868173258003765| 40.22975517890772|56.35969868173258|55.685499058380415|\n",
      "|  Free|  41.6944837340877|41.675035360678926|58.23373408769449|  58.2963224893918|\n",
      "+------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Distribution of Customers By Level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_df = [level: int, Female: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[level: int, Female: bigint ... 1 more field]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = \n",
    "(cust_df.select(col(\"level\"), when(col(\"gender\") === 0, \"Male\").otherwise(\"Female\").alias(\"gender\"))\n",
    "    .groupBy(col(\"level\"))\n",
    "    .pivot(\"gender\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"level\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+\n",
      "|level|Female|Male|\n",
      "+-----+------+----+\n",
      "|    2|   201| 330|\n",
      "|    1|   670| 971|\n",
      "|    0|  1145|1683|\n",
      "+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Top 10 Zip Codes: Which regions consume most from this service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_df = [zip: int, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[zip: int, count: bigint]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = cust_df.groupBy(\"zip\").count().orderBy(desc(\"count\")).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  zip|count|\n",
      "+-----+-----+\n",
      "| 5341|    4|\n",
      "|80821|    4|\n",
      "|71458|    3|\n",
      "|31409|    3|\n",
      "|70446|    3|\n",
      "|20098|    3|\n",
      "|80459|    3|\n",
      "|57445|    3|\n",
      "|78754|    3|\n",
      "|47577|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Distribution of Customers By SignUp Campaign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "udfIndexToCampaign = UserDefinedFunction(<function1>,StringType,None)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,StringType,None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define a udf to Map from campaign number to actual campaign description\n",
    "val udfIndexToCampaign: UserDefinedFunction = udf((camptype: Int) => {\n",
    "  val campaign_map: Map[Int, String] = Map(0 -> \"None\", 1 -> \"30DaysFree\", 2 -> \"SuperBowl\",  3 -> \"RetailStore\", 4 -> \"WebOffer\")\n",
    "  campaign_map.get(camptype)\n",
    "}, StringType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_df = [campaign: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[campaign: string, count: bigint]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = (cust_df\n",
    "             .select(udfIndexToCampaign(col(\"campaign\")).alias(\"campaign\"))\n",
    "             .groupBy(\"campaign\")\n",
    "             .count()\n",
    "             .orderBy(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   campaign|count|\n",
      "+-----------+-----+\n",
      "|  SuperBowl|  240|\n",
      "|RetailStore|  489|\n",
      "|       None|  608|\n",
      "|   WebOffer|  750|\n",
      "| 30DaysFree| 2913|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Average Unique Track Count By Customer Level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_df = [level: string, avg_unique_track_count: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[level: string, avg_unique_track_count: double]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = (music_df.select(\"customer_id\", \"track_id\")\n",
    "             .groupBy(\"customer_id\")\n",
    "             .agg(countDistinct(\"track_id\").alias(\"unique_track_count\"))\n",
    "             .join(cust_df, music_df(\"customer_id\") === cust_df(\"customer_id\"), \"inner\")\n",
    "             .select(udfIndexTolevel(col(\"level\")).alias(\"level\"), col(\"unique_track_count\"))\n",
    "             .groupBy(\"level\")\n",
    "             .agg(avg(\"unique_track_count\").alias(\"avg_unique_track_count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "| level|avg_unique_track_count|\n",
      "+------+----------------------+\n",
      "|Silver|     170.2772699573431|\n",
      "|  Gold|    166.85310734463278|\n",
      "|  Free|     170.9515558698727|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 Mobile Tracks Count By Customer Level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_df = [level: string, avg_mobile_track_count: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[level: string, avg_mobile_track_count: double]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = (music_df.select(\"customer_id\", \"track_id\")\n",
    "             .filter(col(\"is_mobile\") === 1)\n",
    "             .groupBy(\"customer_id\")\n",
    "             .count()\n",
    "             .withColumnRenamed(\"count\", \"mobile_track_count\")\n",
    "             .join(cust_df, music_df(\"customer_id\") === cust_df(\"customer_id\"), \"inner\")\n",
    "             .select(udfIndexTolevel(col(\"level\")).alias(\"level\"), col(\"mobile_track_count\"))\n",
    "             .groupBy(\"level\")\n",
    "             .agg(avg(\"mobile_track_count\").alias(\"avg_mobile_track_count\"))\n",
    "             .orderBy(\"avg_mobile_track_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "| level|avg_mobile_track_count|\n",
      "+------+----------------------+\n",
      "|  Free|    100.01308345120226|\n",
      "|Silver|     146.1614868982328|\n",
      "|  Gold|    160.22033898305085|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Destroying the Spark Session & Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[event_id: int, customer_id: int ... 2 more fields]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_df.unpersist()\n",
    "cust_df.unpersist()\n",
    "click_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toree_spark - Scala",
   "language": "scala",
   "name": "toree_spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
